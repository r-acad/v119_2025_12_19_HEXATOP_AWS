# HEXA TopOpt: AWS Deployment & User Guide

This guide details how to deploy and run the **HEXA TopOpt** code on Amazon Web Services (AWS). Because this codebase relies heavily on GPU acceleration (`CUDA.jl`) and high-performance linear algebra, selecting the correct instance type and environment is critical.

## 1. Prerequisites

Before starting, ensure you have:
* An active **AWS Account**.
* **SSH Client** (Terminal on Mac/Linux, PuTTY or PowerShell on Windows).
* The source code files organized locally on your machine.

---

## 2. Infrastructure Selection (EC2)

The code contains specific optimizations for different GPU architectures (V100, A100, H100). You must match the AWS Instance type to the `gpu_profile` in your config.

### Recommended Instance Types

| Config Profile | AWS Instance Family | GPU Architecture | Precision | Use Case |
| :--- | :--- | :--- | :--- | :--- |
| **"V100"** | **p3.2xlarge** | Tesla V100 (16GB) | Float64 | Standard high-performance runs. |
| **"A100"** | **p4d.24xlarge*** | A100 (40GB) | Float64 | Massive meshes (>50M elements). |
| **"H100"** | **p5.48xlarge*** | H100 (80GB) | Float64 | Extreme scale simulations. |
| **"RTX"** | **g4dn / g5** | T4 / A10G | Float32 | Dev/Test or smaller budgets. |

*\*Note: P4 and P5 instances are expensive and often require a quota increase request from AWS Support.*

### Launching the Instance
1.  **AMI Selection:** Search for **"Deep Learning AMI GPU PyTorch"** (Ubuntu or Amazon Linux 2).
    * *Why?* It comes with NVIDIA Drivers pre-installed, saving hours of setup time.
2.  **Storage:** Increase the Root Volume size to at least **100 GB** (GP3 type). The mesh generation and VTK output files can become very large.
3.  **Key Pair:** Ensure you download the `.pem` key pair for SSH access.

---

## 3. Environment Setup

Once the instance is running, connect via SSH:

```bash
ssh -i /path/to/key.pem ubuntu@<instance-public-ip>
```

### Step A: Install Julia
The AWS AMI has Python/CUDA, but not Julia. Run the following to install the latest version via `juliaup`:

```bash
curl -fsSL [https://install.julialang.org](https://install.julialang.org) | sh
# Press Enter to accept defaults
source ~/.bashrc
```

### Step B: Upload the Code
On your **local machine**, organize your files into a folder named `HEXA_TopOpt`. The structure must look like this:

```text
HEXA_TopOpt/
├── Run.jl
├── Project.toml (Generated by the provided script, or let Run.jl create it)
├── src/
│   ├── Main.jl
│   ├── Core/
│   ├── Mesh/
│   ├── Solvers/
│   ├── Optimization/
│   ├── IO/
│   └── Utils/
└── configs/
    └── default.yaml
```

Upload this folder to the AWS instance using `scp`:

```bash
scp -i /path/to/key.pem -r path/to/local/HEXA_TopOpt ubuntu@<instance-public-ip>:~/
```

---

## 4. Running the Simulation

Connect to your instance and navigate to the folder:

```bash
cd ~/HEXA_TopOpt
```

### Option A: Interactive Run (Testing)
For a quick test to ensure the environment builds correctly:

```bash
julia Run.jl
```
*Note: The first run will trigger `setup_robust_environment()` which installs dependencies (CUDA, Krylov, etc.) and precompiles them. This may take 5-10 minutes.*

### Option B: Production Run (Background)
For long simulations, use `nohup` so the process continues if your SSH connection drops:

```bash
nohup julia Run.jl configs/default.yaml > run_output.log 2>&1 &
```

You can monitor progress by tailing the log:

```bash
tail -f run_output.log
```

---

## 5. Configuration Guide (`default.yaml`)

You **must** edit `configs/default.yaml` to match your chosen AWS instance.

**For a p3.2xlarge (Tesla V100):**
```yaml
gpu_profile: "V100"
solver_parameters:
  solver_type: gpu
  preconditioner: "multigrid"
  tolerance: 1.0e-9
```

**For a g4dn.xlarge (Tesla T4 - Budget Option):**
```yaml
gpu_profile: "RTX"  # Forces Float32 to save memory
growth_settings:
  max_background_elements: 80_000_000 # Lower limit for smaller VRAM
```

**For Memory Safety:**
If the solver crashes with "VRAM Exhausted", lower the `target_elem_count` or `target_active_elements` in the config file.

---

## 6. Retrieving Results

The code automatically creates a `RESULTS` directory. To download the simulation logs, VTK files (for ParaView), and STL files to your local machine:

```bash
# Run this on your LOCAL machine
scp -i /path/to/key.pem -r ubuntu@<instance-public-ip>:~/HEXA_TopOpt/RESULTS ./local_results_folder
```

### Visualizing Results
1.  **VTK Files:** Open the `.vti` or `.vtk` files in [ParaView](https://www.paraview.org/).
2.  **STL Files:** Can be opened in any CAD software or Windows 3D Viewer.
3.  **Logs:** `simulation_log.txt` contains CSV-like data on compliance and volume fraction.

---

## 7. Troubleshooting

**Error: `OutOfMemoryError()` / `VRAM Exhausted`**
* **Cause:** The mesh is too fine for the GPU memory.
* **Fix:** Edit `configs/default.yaml`. Under `growth_settings`, reduce `target_active_elements` (e.g., from 25M to 10M). Alternatively, set `solver_parameters -> preconditioner: "jacobi"` (uses ~30% less RAM than multigrid).

**Error: `Project.toml not found`**
* **Fix:** The `Run.jl` script has a self-healing mechanism. It will detect this and attempt to generate a new environment. Just let it run; if it fails, ensure `src/Utils/get_package_versions.jl` is present.

**Simulation freezes (Stagnation)**
* **Fix:** In `configs/default.yaml`, increase `stagnation_tolerance` to `1.0e-2` or switch `solver_type` to `matrix_free` (CPU) if the GPU numerical precision is insufficient.

---

## 8. Cost Management (Important)

GPU instances are costly (e.g., p3.2xlarge is ~$3.00/hr).
1.  **Stop the Instance:** When the simulation finishes, stop the instance via the AWS Console to stop billing for compute.
2.  **Terminate:** If you have downloaded your results and no longer need the setup, **Terminate** the instance to stop billing for storage (EBS volumes).
